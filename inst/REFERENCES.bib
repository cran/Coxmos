%% COX
@Article{Cox_1972,
title={Regression models and life tables (with discussion},
author={David Cox},
journal={Royal Statistical Society},
url={https://doi.org/10.1111/j.2517-6161.1972.tb00899.x},
year={1972},
mag_id={2995133996},
abstract={The drum mallets disclosed are adjustable, by the percussion player, as to weight and/or balance and/or head characteristics, so as to vary the "feel" of the mallet, and thus also the tonal effect obtainable when playing upon kettle-drums, snare-drums, and other percussion instruments; and, typically, the mallet has frictionally slidable, removable and replaceable, external balancing mass means, positionable to serve as the striking head of the mallet, whereby the adjustment as to balance, overall weight, head characteristics and tone production may be readily Bastien_2008obtained. In some forms, the said mass means regularly serves as a removable and replaceable striking head; while in other forms, the mass means comprises one or more thin elongated tubes having a frictionally-gripping fit on an elongated mallet body, so as to be manually slidable thereon but tight enough to avoid dislodgment under normal playing action; and such a tubular member may be slidable to the head-end of the mallet to serve as a striking head or it may be slidable to a position to serve as a hand grip; and one or more such tubular members may be placed in various positions along the length of the mallet. The mallet body may also have a tKaplan_1958apered element at the head-end to assure retention of mass members especially of enlarged-head types; and the disclosure further includes such heads embodying a relatively hard inner portion and a relatively soft outer covering.}}

%% EPV

@Article{Concato_1995,
title={Importance of events per independent variable in proportional hazards analysis I. Background, goals, and general strategy},
year={1995},
author={John Concato and Peter Peduzzi and Theodore R. Holford and Alvan R. Feinstein},
doi={10.1016/0895-4356(95)00510-2},
url={https://pubmed.ncbi.nlm.nih.gov/8543963/},
pmid={8543963},
mag_id={1982245010},
journal={Journal of Clinical Epidemiology},
abstract={Abstract   Multivariable methods of analysis can yield problematic results if methodological guidelines and mathematical assumptions are ignored. A problem arising from a too-small ratio of events per variable (EPV) can affect regression coefficients' accuracy, precision and their statistical significance tests. The problem occurs when a proportional hazards analysis contains too few “failure” events (e.g., deaths) in relation to the number of included independent variables. In the current research, the impact of EPV was assessed for results of proportional hazards analysis done with Monte Carlo simulations in an empirical data set of 673 subjects enrolled in a multicenter trial of coronary artery bypass surgery.  The research is presented in two parts: Part I describes the data set and strategy used for the analyses, including the Monte Carlo simulation studies done to determine and compare the impact of various values of EPV in proportional hazards analytical results. Part II compares the output of regression models obtained from the simulations, and discusses the implication of the findings.}}

@Article{Peduzzi_1995,
title={Importance of events per independent variable in proportional hazards regression analysis. II. Accuracy and precision of regression estimates},
year={1995},
author={Peter Peduzzi and John Concato and Alvan R. Feinstein and Theodore R. Holford},
doi={https://doi.org/10.1016/0895-4356(95)00048-8},
pmid={8543964},
pmcid={null},
mag_id={2008706768},
journal={Journal of Clinical Epidemiology},
abstract={Abstract   The analytical effect of the number of events per variable (EPV) in a proportional hazards regression analysis was evaluated using Monte Carlo simulation techniques for data from a randomized trial containing 673 patients and 252 deaths, in which seven predictor variables had an original significance level of  p    Five hundred simulated analyses were conducted for these seven variables at EPVs of 2, 5, 10, 15, 20, and 25. For each simulation, a random exponential survival time was generated for each of the 673 patients, and the simulated results were compared with their original counterparts. As EPV decreased, the regression coefficients became more biased relative to the true value; the 90\% confidence limits about the simulated values did not have a coverage of 90\% for the original value; large sample properties did not hold for variance estimates from the proportional hazards model, and the  Z  statistics used to test the significance of the regression coefficients lost validity under the null hypothesis.  Although a single boundary level for avoiding problems is not easy to choose, the value of EPV = 10 seems most prudent. Below this value for EPV, the results of proportional hazards regression analyses should be interpreted with caution because the statistical model may not be valid.}}

@Article{Vittinghoff_2007,
title={Relaxing the Rule of Ten Events per Variable in Logistic and Cox Regression},
year={2007},
author={Eric Vittinghoff and Charles E. McCulloch},
doi={https://doi.org/10.1093/aje/kwk052},
pmid={17182981},
pmcid={null},
mag_id={2130373985},
journal={American Journal of Epidemiology},
abstract={The rule of thumb that logistic and Cox models should be used with a minimum of 10 outcome events per predictor variable (EPV), based on two simulation studies, may be too conservative. The authors conducted a large simulation study of other influences on confidence interval coverage, type I error, relative bias, and other model performance measures. They found a range of circumstances in which coverage and bias were within acceptable levels despite less than 10 EPV, as well as other factors that were as influential as or more influential than EPV. They conclude that this rule can be relaxed, in particular for sensitivity analyses undertaken to demonstrate adequate control of confounding. bias (epidemiology); coverage probability; event history analysis; model adequacy; type I error; variable selection}}

%% COX RESIDUALS
@Article{Schoenfeld_1982,title={Partial residuals for the proportional hazards regression model},
year={1982},
author={David A. Schoenfeld},
doi={https://doi.org/10.1093/biomet/69.1.239},
url={https://academic.oup.com/biomet/article-abstract/69/1/239/243012?redirectedFrom=fulltext},
pmid={null},
pmcid={null},
mag_id={1977247044},
journal={Biometrika},
abstract={null}}

@Article{Grambsch_1994,title={Proportional hazards tests and diagnostics based on weighted residuals},
year={1994},
author={Patricia M. Grambsch and Terry M. Therneau},
doi={https://doi.org/10.1093/biomet/81.3.515},
url={https://academic.oup.com/biomet/article-abstract/81/3/515/257037?redirectedFrom=fulltext},
pmid={null},
pmcid={null},
mag_id={1966714873},
journal={Biometrika},
abstract={SUMMARY Nonproportional hazards can often be expressed by extending the Cox model to include time varying coefficients; e.g., for a single covariate, the hazard function for subject i is modelled as exp { fl(t)Zi(t)}. A common example is a treatment effect that decreases with time. We show that the function /3(t) can be directly visualized by smoothing an appropriate residual plot. Also, many tests of proportional hazards, including those of Cox (1972), Gill & Schumacher (1987), Harrell (1986), Lin (1991), Moreau, O'Quigley & Mesbah (1985), Nagelkerke, Oosting & Hart (1984), O'Quigley & Pessione (1989), Schoenfeld (1980) and Wei (1984) are related to time-weighted score tests of the proportional hazards hypothesis, and can be visualized as a weighted least-squares line fitted to the residual plot.}}

%% Kaplan Meier
@Article{Kaplan_1958,
title={Nonparametric Estimation from Incomplete Observations},
year={1958},
author={Edward L. Kaplan and E. L. Kaplan and Paul Meier},
doi={https://doi.org/10.1007/978-1-4612-4380-9_25},
url={https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_25},
pmid={null},
pmcid={null},
mag_id={1979300931},
journal={Journal of the American Statistical Association},
abstract={Abstract In lifetesting, medical follow-up, and other fields the observation of the time of occurrence of the event of interest (called a death) may be prevented for some of the items of the sample by the previous occurrence of some other event (called a loss). Losses may be either accidental or controlled, the latter resulting from a decision to terminate certain observations. In either case it is usually assumed in this paper that the lifetime (age at death) is independent of the potential loss time; in practice this assumption deserves careful scrutiny. Despite the resulting incompleteness of the data, it is desired to estimate the proportion P(t) of items in the population whose lifetimes would exceed t (in the absence of such losses), without making any assumption about the form of the function P(t). The observation for each item of a suitable initial event, marking the beginning of its lifetime, is presupposed. For random samples of size N the product-limit (PL) estimate can be defined as follows: L...}}

%% cox R
@Manual{survival_package,
    title = {A Package for Survival Analysis in R},
    author = {Terry M Therneau},
    year = {2024},
    note = {R package version 3.5-8},
    url = {https://CRAN.R-project.org/package=survival},
}

@Manual{survminer_package,
    title = {survminer: Drawing Survival Curves using 'ggplot2'},
    author = {Alboukadel Kassambara and Marcin Kosinski and Przemyslaw Biecek},
    year = {2021},
    note = {R package version 0.4.9},
    url = {https://CRAN.R-project.org/package=survminer},
  }

%% coxSW
@Article {Efroymson_SW,
    author  = "M. A. Efroymson",
    title   = "Multiple Regression Analysis",
    journal = "Mathematical Methods for Digital Computers",
    year    = "1960"
}

@misc{coxSW_CRAN,
  author = {International-Harvard Statistical Consulting Company},
  title = {My.stepwise: Stepwise Variable Selection Procedures for Regression Analysis},
  year = 2017,
  month = 06,
  day = 29,
  url = {https://cran.r-project.org/package=My.stepwise}
}

%% coxEN
@Article{Simon_2011,title={Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent},
year={2011},
author={Noah Simon and Jerome H. Friedman and Jerome H. Friedman and Trevor Hastie and Robert Tibshirani},
doi={10.18637/jss.v039.i05},
url={https://pubmed.ncbi.nlm.nih.gov/27065756/},
pmid={27065756},
pmcid={4824408},
mag_id={2157076315},
journal={Journal of Statistical Software},
abstract={We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of l1 and l2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.}}

%% plsRcox

@Article{Bastien_2005,title={PLS generalised linear regression},
year={2005},
author={Philippe Bastien and Vincenzo Esposito Vinzi and Michel Tenenhaus},
doi={},
url={https://www.sciencedirect.com/science/article/abs/pii/S0167947304000271?via%3Dihub},
pmid={null},
pmcid={null},
mag_id={1993895874},
journal={Computational Statistics \& Data Analysis},
abstract={PLS univariate regression is a model linking a dependent variable y to a set X = {x1 ;:::; xp} of (numerical or categorical) explanatory variables. It can be obtained as a series of simple and multiple regressions. By taking advantage from the statistical tests associated with linear regression, it is feasible to select the signi3cant explanatory variables to include in PLS regression and to choose the number of PLS components to retain. The principle of the presented algorithm may be similarly used in order to yield an extension of PLS regression to PLS generalised linear regression. The modi3cations to classical PLS regression, the case of PLS logistic regression and the application of PLS generalised linear regression to survival data are studied in detail. Some examples show the use of the proposed methods in real practice. As a matter of fact, classical PLS univariate regression is the result of an iterated use of ordinary least squares (OLS) where PLS stands for partial least squares. PLS generalised linear regression retains the rationale of PLS while the criterion optimised at each step is based on maximum likelihood. Nevertheless, the acronym PLS is kept as a reference to a general methodology for relating a response variable to a set of predictors. The approach proposed for PLS generalised linear regression is simple and easy to implement. Moreover, it can be easily generalised to any model that is linear at the level of the explanatory variables. c}}

@Article{Bastien_2008,title={Deviance residuals based PLS regression for censored data in high dimensional setting},
year={2008},
author={Philippe Bastien},
doi={10.1016/j.chemolab.2007.09.009},
url={https://www.sciencedirect.com/science/article/abs/pii/S0169743907001931?via%3Dihub},
pmid={null},
pmcid={null},mag_id={1998842747},
journal={Chemometrics and Intelligent Laboratory Systems},
abstract={Abstract   The PLS Cox regression has been proposed in the framework of PLS generalized linear regression as an alternative to the Cox model when dealing with highly correlated covariates. However, in high dimensional settings the algorithm becomes computer-intensive and a more efficient algorithm must be used. In this Article we propose an alternative both faster and easier to carry out by the direct use of standard procedures which are available in most statistical softwares. Recently, Segal suggested a solution to the Cox–Lasso algorithm when dealing with high dimensional data. Following Segal, we propose a Deviance Residuals based PLS regression (PLSDR) as an alternative to the PLS–Cox model in high dimensional settings. The PLSDR algorithm only needs to carry out null deviance residuals using a simple intercept Cox model and use these as outcome in a standard PLS regression. This algorithm which can be extended to kernels to deal with non-linearity can also be viewed as a variable selection method in a threshold penalized formulation. An application carried out on gene expression from patients with diffuse large B-cell lymphoma shows the practical interest of using deviance residuals as outcomes in PLS regression when dealing with very many descriptors and censored data.}}

@Article{Bastien_2015,title={Deviance residuals-based sparse PLS and sparse kernel PLS regression for censored data},
year={2015},
author={Philippe Bastien and Philippe Bastien and Frédéric Bertrand and Nicolas Meyer and Nicolas Meyer and Nicolas Meyer and Myriam Maumy-Bertrand},
doi={},
url={https://academic.oup.com/bioinformatics/article/31/3/397/2366078},
pmid={25286920},
pmcid={null},
mag_id={2152644043},
journal={Bioinformatics},
abstract={Motivation: A vast literature from the past decade is devoted to relating gene profiles and subject survival or time to cancer recurrence. Biomarker discovery from high-dimensional data, such as transcrip-tomic or single nucleotide polymorphism profiles, is a major challenge in the search for more precise diagnoses. The proportional hazard regression model suggested by Cox (1972), to study the relationship between the time to event and a set of covariates in the presence of censoring is the most commonly used model for the analysis of survival data. However, like multivariate regression, it supposes that more observations than variables, complete data, and not strongly correlated variables are available. In practice, when dealing with high-dimensional data, these constraints are crippling. Collinearity gives rise to issues of over-fitting and model misidentification. Variable selection can improve the estimation accuracy by effectively identifying the subset of relevant predictors and enhance the model interpretability with parsimonious representation. To deal with both collinearity and variable selection issues, many methods based on least absolute shrinkage and selection operator penalized Cox proportional hazards have been proposed since the reference paper of Tibshirani. Regularization could also be performed using dimension reduction as is the case with partial least squares (PLS) regression. We propose two original algorithms named sPLSDR and its non-linear kernel counterpart DKsPLSDR, by using sparse PLS regression (sPLS) based on deviance residuals. We compared their predicting performance with state-of-the-art algorithms on both simulated and real reference benchmark datasets. Results: sPLSDR and DKsPLSDR compare favorably with other methods in their computational time, prediction and selectivity, as indicated by results based on benchmark datasets. Moreover, in the framework of PLS regression, they feature other useful tools, including biplots representation, or the ability to deal with missing data. Therefore, we view them as a useful addition to the toolbox of estimation and prediction methods for the widely used Cox's model in the high-dimensional and low-sample size settings. Availability and implementation: The R-package plsRcox is available on the CRAN and is maintained by Fr ed eric Bertrand.}}

%% AUC Evaluation COX
@booklet{survivalROCdiscussion,
   title = {Introduction to survivalROC: An R Package for Survival-ROC Method},
   author = {P. Saha and P. J. Heagerty}
}

@Article{survivalROC,
   abstract = {ROC curves are a popular method for displaying sensitivity and specificity of a continuous diagnostic marker, X , for a binary disease variable, D. However, many disease outcomes are time dependent, D (t) , and ROC curves that vary as a function of time may be more appropriate. A common example of a time-dependent variable is vital status, where D (t) = 1 if a patient has died prior t o time t and zero otherwise. We propose summarizing the discrimination potential of a marker X , measured at baseline (t = O), by calculating ROC curves for cumulative disease or death incidence by time t , which we denote as ROC(t). A typical complexity with survival data is that observations may be censored. Two ROC curve estimators are proposed that can accommodate censored data. A simple estimator is based on using the Kaplan-Meier estimator for each possible subset X > c. However, this estimator does not guarantee the necessary condition that sensitivity and specificity are monotone in X. An alternative estimator that does guarantee monotonicity is based on a nearest neighbor estimator for the bivariate distribution function of (X , T) , where T represents survival time (Akritas, M. J., 1994, Annuls of Statistics 22, 1299-1327). We present an example where ROC(t) is used to compare a standard and a modified flow cytometry measurement for predicting survival after detection of breast cancer and an example where the ROC(t) curve displays the impact of modifying eligibility criteria for sample size and power in HIV prevention trials.},
   author = {Patrick J. Heagerty and Thomas Lumley and Margaret S. Pepe},
   keywords = {Accuracy,Discrimination,Kaplan-Meier estimator,Kernel smoothing,Sensitivity,Speci-ficity},
   title = {Time-Dependent ROC Curves for Censored Survival Data and a Diagnostic Marker},
   year = {2000},
   journal = {Biometrics}
}

@booklet{risksetROCdiscussion,
   title = {Introduction to risksetROC: An R Package for Riskset-ROC (Incident/Dynamic) Calculation},
   author = {P. Saha and P. J. Heagerty and Yingye Zheng},
   year = {2006},
}

@Article{risksetROC,
   abstract = {The predictive accuracy of a survival model can be summarized using extensions of the proportion of variation explained by the model, or R 2 , commonly used for continuous response models, or using extensions of sensitivity and specificity, which are commonly used for binary response models. In this Article we propose new time-dependent accuracy summaries based on time-specific versions of sensitivity and specificity calculated over risk sets. We connect the accuracy summaries to a previously proposed global concordance measure, which is a variant of Kendall's tau. In addition, we show how standard Cox regression output can be used to obtain estimates of time-dependent sensitivity and specificity, and time-dependent receiver operating characteristic (ROC) curves. Semiparametric estimation methods appropriate for both proportional and nonproportional hazards data are introduced, evaluated in simulations, and illustrated using two familiar survival data sets.},
   author = {Patrick J Heagerty and Yingye Zheng},
   journal = {Biometrics},
   keywords = {Cox regression,Discrimination,Prediction,Sensitivity,Specificity},
   pages = {92-105},
   title = {Survival Model Predictive Accuracy and ROC Curves},
   volume = {61},
   year = {2005},
   doi = {https://doi.org/10.1111/j.0006-341x.2005.030814.x},
}

@Article{cenROC,
   abstract = {The prediction reliability is of primary concern in many clinical studies when the objective is to develop new predictive models or improve existing risk scores. In fact, before using a model in any clinical decision making, it is very important to check its ability to discriminate between subjects who are at risk of, for example, developing certain disease in a near future from those who will not. To that end, the time-dependent receiver operating characteristic (ROC) curve is the most commonly used method in practice. Several approaches have been proposed in the literature to estimate the ROC nonparametrically in the context of survival data. But, except one recent approach, all the existing methods provide a nonsmooth ROC estimator whereas, by definition, the ROC curve is smooth. In this Article we propose and study a new nonparametric smooth ROC estimator based on a weighted kernel smoother. More precisely, our approach relies on a well-known kernel method used to estimate cumulative distribution functions of random variables with bounded supports. We derived some asymptotic properties for the proposed estimator. As bandwidth is the main parameter to be set, we present and study different methods to appropriately select one. A simulation study is conducted, under different scenarios, to prove the consistency of the proposed method and to compare its finite sample performance with a competitor. The results show that the proposed method performs better and appear to be quite robust to bandwidth choice. As for inference purposes, our results also reveal the good performances of a proposed nonparametric bootstrap procedure. Furthermore, we illustrate the method using a real data example.},
   author = {Kassu Mehari Beyene and Anouar El Ghouch},
   doi = {},
   url = {https://pubmed.ncbi.nlm.nih.gov/32687225/},
   issn = {10970258},
   issue = {24},
   journal = {Statistics in Medicine},
   keywords = {AUC,bandwidth selection,kernel estimation,sensitivity and specificity,weighted distribution},
   month = {10},
   pages = {3373-3396},
   pmid = {32687225},
   publisher = {John Wiley and Sons Ltd},
   title = {Smoothed time-dependent receiver operating characteristic curve for right censored survival data},
   volume = {39},
   year = {2020},
}

@Article{nsROC,
   abstract = {The receiver operating characteristic (ROC) curve is a graphical method which has become standard in the analysis of diagnostic markers, that is, in the study of the classification ability of a numerical variable. Most of the commercial statistical software provide routines for the standard ROC curve analysis. Of course, there are also many R packages dealing with the ROC estimation as well as other related problems. In this work we introduce the nsROC package which incorporates some new ROC curve procedures. Particularly: ROC curve comparison based on general distances among functions for both paired and unpaired designs; efficient confidence bands construction; a generalization of the curve considering different classification subsets than the one involved in the classical definition of the ROC curve; a procedure to deal with censored data in cumulative-dynamic ROC curve estimation for time-to-event outcomes; and a non-parametric ROC curve method for meta-analysis. This is the only R package which implements these particular procedures.},
   author = {Sonia Pérez-Fernández and Pablo Martínez-Camblor and Peter Filzmoser and Norberto Corral},
   title = {nsROC: An R package for Non-Standard ROC Curve Analysis},
   year = {2018},
   doi = {https://doi.org/10.1007/s00180-020-00955-7},
   journal = {The R Journal},
}

@Article{smoothROCtime,
   abstract = {The receiver operating characteristic curve is a popular graphical method often used to study the diagnostic capacity of continuous (bio)markers. When the considered outcome is a time-dependent variable, two main extensions have been proposed: the cumulative/dynamic receiver operating characteristic curve and the incident/dynamic receiver operating characteristic curve. In both cases, the main problem for developing appropriate estimators is the estimation of the joint distribution of the variables time-to-event and marker. As usual, different approximations lead to different estimators. In this Article, the authors explore the use of a bivariate kernel density estimator which accounts for censored observations in the sample and produces smooth estimators of the time-dependent receiver operating characteristic curves. The performance of the resulting cumulative/dynamic and incident/dynamic receiver operating characteristic curves is studied by means of Monte Carlo simulations. Additionally, the influence of the choice of the required smoothing parameters is explored. Finally, two real-applications are considered. An R package is also provided as a complement to this Article.},
   author = {Pablo Martínez-Camblor and Juan Carlos Pardo-Fernández},
   doi = {https://doi.org/10.1177/0962280217740786},
   issn = {14770334},
   issue = {3},
   journal = {Statistical Methods in Medical Research},
   keywords = {Censoring,discrimination,kernel density estimator,receiver operating characteristic curve,sensitivity,specificity},
   month = {3},
   pages = {651-674},
   pmid = {29187044},
   publisher = {SAGE Publications Ltd},
   title = {Smooth time-dependent receiver operating characteristic curve estimators},
   volume = {27},
   year = {2018},
}


@Article{smoothROCtime_RPackage,
   abstract = {The receiver operating characteristic (ROC) curve has become one of the most used tools for analyzing the diagnostic capacity of continuous biomarkers. When the studied outcome is a time-dependent variable two main generalizations have been proposed, based on properly extensions of the sensitivity and the specificity. Different procedures have been suggested for their estimation mainly under the presence of right censorship. Most of them have been implemented, as well, in diverse types of software, including R packages. This work focuses on the R implementation for the smooth estimation of time-dependent ROC curves. The theoretical connection between them through the joint distribution function of the biomarker and time-to-event variables prompts an approximation method: considered estimators are based on the bivariate kernel density estimator for the joint density function of the bidimensional variable (Marker, Time-to-event). The use of the package is illustrated with two real-world examples.},
   author = {Susana Díaz-Coto and Pablo Martínez-Camblor and Sonia Pérez-Fernández},
   doi = {https://doi.org/10.1007/s00180-020-00955-7},
   issn = {16139658},
   issue = {3},
   journal = {Computational Statistics},
   keywords = {(Bio)markers,Area under the curve,Smooth time-dependent ROC curve estimation,Time-dependent ROC curve,Time-dependent outcomes},
   month = {9},
   pages = {1231-1251},
   publisher = {Springer},
   title = {smoothROCtime: an R package for time-dependent ROC curve estimation},
   volume = {35},
   year = {2020},
}

@Article{survcomp_IBRIER,
    title = {survcomp: an R/Bioconductor package for performance assessment and comparison of survival models},
    author = {Schroeder MS and Culhane AC and Quackenbush J and Haibe-Kains B},
    journal = {Bioinformatics},
    year = {2011},
    volume = {27(22)},
    pages = {3206-3208.},
  }

%% Methods Evaluation COX
@Article{C_INDEX,
   author = {Frank E Harrell and Robert M Califf and David B Pryor and Kerry L Lee and Robert A Rosati},
   title = {Evaluating the Yield of Medical Tests},
   url = {https://jamanetwork.com/journals/jama},
   journal = {JAMA},
   doi = {https://doi.org/10.1001/jama.1982.03320430047030},
   volume = {247},
   year = 1982,
}

@Book{ggplot2,
    author = {Hadley Wickham},
    title = {ggplot2: Elegant Graphics for Data Analysis},
    publisher = {Springer-Verlag New York},
    year = {2016},
    isbn = {978-3-319-24277-4},
    url = {https://ggplot2.tidyverse.org/},
  }

%% TESTS

@Article{Wilcoxon,
    author = { David F. Bauer },
    title = {Constructing Confidence Sets Using Rank Statistics},
    journal = {Journal of the American Statistical Association},
    volume = {67},
    number = {339},
    pages = {687-690},
    year  = {1972},
    publisher = {Taylor and Francis},
    doi = {https://doi.org/10.1080/01621459.1972.10481279},
    URL = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1972.10481279},
}

@book{Chi2,
  title={Mathematics of Statistics},
  author={Kenney, J.F. and Keeping, E.S.},
  url={https://books.google.es/books?id=AH4GxAEACAAJ},
  year={1951},
  publisher={Van Nostrand}
}

@article{MixOmics,
   abstract = {The advent of high throughput technologies has led to a wealth of publicly available ‘omics data coming from different sources, such as transcriptomics, proteomics, metabolomics. Combining such large-scale biological data sets can lead to the discovery of important biological insights, provided that relevant information can be extracted in a holistic manner. Current statistical approaches have been focusing on identifying small subsets of molecules (a ‘molecular signature’) to explain or predict biological conditions, but mainly for a single type of ‘omics. In addition, commonly used methods are univariate and consider each biological feature independently. We introduce mixOmics, an R package dedicated to the multivariate analysis of biological data sets with a specific focus on data exploration, dimension reduction and visualisation. By adopting a systems biology approach, the toolkit provides a wide range of methods that statistically integrate several data sets at once to probe relationships between heterogeneous ‘omics data sets. Our recent methods extend Projection to Latent Structure (PLS) models for discriminant analysis, for data integration across multiple ‘omics data or across independent studies, and for the identification of molecular signatures. We illustrate our latest mixOmics integrative frameworks for the multivariate analyses of ‘omics data available from the package.},
   author = {Florian Rohart and Benoît Gautier and Amrit Singh and Kim Anh Lê Cao},
   doi = {},
   url = {https://pubmed.ncbi.nlm.nih.gov/29099853/},
   issn = {15537358},
   issue = {11},
   journal = {PLoS Computational Biology},
   month = {11},
   pmid = {29099853},
   publisher = {Public Library of Science},
   title = {mixOmics: An R package for ‘omics feature selection and multiple data integration},
   volume = {13},
   year = {2017},
}
